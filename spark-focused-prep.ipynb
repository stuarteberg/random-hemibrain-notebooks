{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from itertools import chain\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "#tqdm = tqdm_notebook\n",
    "\n",
    "import vigra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dvidutils import LabelMapper\n",
    "from libdvid import DVIDNodeService\n",
    "\n",
    "from neuclease.dvid import *\n",
    "from neuclease.util import Timer\n",
    "from neuclease.misc import find_best_plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DVIDSparkServices.spark_launch_scripts.janelia_lsf.lsf_utils import get_hostgraph_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler(sys.stdout)\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.handlers = []\n",
    "root_logger.addHandler(handler)\n",
    "root_logger.setLevel(logging.INFO)\n",
    "logging.getLogger('kafka').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nrs/flyem/bergs/complete-ffn-agglo\n"
     ]
    }
   ],
   "source": [
    "cd /nrs/flyem/bergs/complete-ffn-agglo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h10u30.int.janelia.org\r\n"
     ]
    }
   ],
   "source": [
    "!uname -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nrs/flyem/bergs/complete-ffn-agglo'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://h10u12:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://h02u03:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://h02u03:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_approach(sv_vol, id_a, id_b):\n",
    "    \"\"\"\n",
    "    Given a segmentation volume and two label IDs which it contains,\n",
    "    Find the two coordinates within id_a and id_b, respectively,\n",
    "    which mark the two objects' closest approach, i.e. where the objects\n",
    "    come closest to touching, even if they don't actually touch.\n",
    "    \n",
    "    Returns (coord_a, coord_b)\n",
    "    \"\"\"\n",
    "    # For all voxels, find the shortest vector toward id_b\n",
    "    to_b_vectors = vigra.filters.vectorDistanceTransform((sv_vol == id_b).astype(np.uint32))\n",
    "\n",
    "    # Magnitude of those vectors == distance to id_b\n",
    "    to_b_distances = np.linalg.norm(to_b_vectors, axis=-1)\n",
    "\n",
    "    # We're only interested in the voxels within id_a;\n",
    "    # everything else is infinite distance\n",
    "    to_b_distances[sv_vol != id_a] = np.inf\n",
    "\n",
    "    # Find the point within id_a with the smallest vector\n",
    "    point_a = np.unravel_index(np.argmin(to_b_distances), to_b_distances.shape)\n",
    "\n",
    "    # Its closest point id_b is indicated by the corresponding vector\n",
    "    point_b = (point_a + to_b_vectors[point_a]).astype(int)\n",
    "\n",
    "    return (point_a, point_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_events_to_mapping(split_events, leaves_only=False):\n",
    "    \"\"\"\n",
    "    Convert the given split_events,\n",
    "    into a mapping, from all split fragment supervoxel IDs to their ROOT supervoxel ID,\n",
    "    i.e. the supervoxel from which they came originally.\n",
    "\n",
    "    Args:\n",
    "        split_events:\n",
    "            As produced by fetch_supervoxel_splits()\n",
    "\n",
    "        leaves_only:\n",
    "            If True, do not include intermediate supervoxels in the mapping;\n",
    "            only include fragment IDs that have not been further split,\n",
    "            i.e. they still exist in the volume.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series, where index is fragment ID, data is root ID.\n",
    "    \"\"\"\n",
    "    if len(split_events) == 0:\n",
    "        return np.zeros((0,2), np.uint64)\n",
    "    \n",
    "    split_tables = list(map(lambda t: np.asarray(t, np.uint64), split_events.values()))\n",
    "    split_table = np.concatenate(split_tables)\n",
    "\n",
    "    old_svs = split_table[:, SplitEvent._fields.index('old')]\n",
    "    remain_fragment_svs = split_table[:, SplitEvent._fields.index('remain')]\n",
    "    split_fragment_svs = split_table[:, SplitEvent._fields.index('split')]\n",
    "\n",
    "    if leaves_only:\n",
    "        leaf_fragment_svs = (set(remain_fragment_svs) | set(split_fragment_svs)) - set(old_svs)\n",
    "        fragment_svs = np.fromiter(leaf_fragment_svs, np.uint64)\n",
    "    else:\n",
    "        fragment_svs = np.concatenate((remain_fragment_svs, split_fragment_svs))\n",
    "        \n",
    "    g = split_events_to_graph(split_events)\n",
    "    root_svs = np.fromiter(map(lambda sv: find_root(g, sv), fragment_svs), np.uint64, len(fragment_svs))\n",
    "\n",
    "    mapping = pd.Series(index=fragment_svs, data=root_svs)\n",
    "    mapping.index.name = 'fragment_sv'\n",
    "    mapping.name = 'root_sv'\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sanitize_server\n",
    "def expand_uuid(server, uuid, repo_uuid=None):\n",
    "    repo_uuid = repo_uuid or uuid\n",
    "    repo_info = fetch_repo_info(server, repo_uuid)\n",
    "    full_uuids = repo_info[\"DAG\"][\"Nodes\"].keys()\n",
    "    \n",
    "    matching_uuids = list(filter(lambda full_uuid: uuids_match(uuid, full_uuid), full_uuids))\n",
    "    if len(matching_uuids) == 0:\n",
    "        raise RuntimeError(f\"No matching uuid for '{uuid}'\")\n",
    "    \n",
    "    if len(matching_uuids) > 1:\n",
    "        raise RuntimeError(f\"Multiple ({len(matching_uuids)}) uuids match '{uuid}': {matching_uuids}\")\n",
    "\n",
    "    return matching_uuids[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hostgraph URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook:\n",
      "http://lsf-rtm/cacti/plugins/grid/grid_bjobs.php?action=viewjob&tab=hostgraph&clusterid=1&indexid=0&jobid=44140756&submit_time=1532630695\n",
      "Cluster:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MASTER_BJOB_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-647d768d2396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_hostgraph_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LSB_JOBID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cluster:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_hostgraph_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MASTER_BJOB_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/groups/flyem/proj/cluster/miniforge/envs/flyem/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MASTER_BJOB_ID'"
     ]
    }
   ],
   "source": [
    "print(\"This notebook:\")\n",
    "print(get_hostgraph_url(os.environ[\"LSB_JOBID\"]))\n",
    "print(\"Cluster:\")\n",
    "print(get_hostgraph_url(os.environ[\"MASTER_BJOB_ID\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting agglo\n",
    "initial_agglo = DvidInstanceInfo('emdata3:8900', 'ac901', 'segmentation')\n",
    "\n",
    "# The uuid used when loading the neo4j instance (for 'important bodies')\n",
    "neo4j_reference = DvidInstanceInfo('emdata3:8900', '52f9', 'segmentation')\n",
    "\n",
    "# The last supervoxel splits: One past the neo4j node\n",
    "analysis_node = DvidInstanceInfo('emdata3:8900', '662e', 'segmentation')\n",
    "\n",
    "# We won't be using this...\n",
    "current_master = DvidInstanceInfo('emdata3:8900', 'f545', 'segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load split SVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166973 kafka messages took 7.220730543136597 seconds\n",
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166973 kafka messages took 7.392569065093994 seconds\n"
     ]
    }
   ],
   "source": [
    "leaf_fragment_svs, retired_svs = fetch_supervoxel_fragments(analysis_node, 'kafka')\n",
    "retired_svs = set(retired_svs)\n",
    "split_events = fetch_supervoxel_splits(analysis_node, 'kafka')\n",
    "split_mapping = split_events_to_mapping(split_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load neo4j-defined important bodies; append final splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166973 kafka messages took 7.009605646133423 seconds\n"
     ]
    }
   ],
   "source": [
    "# This list was generated from node 52f9\n",
    "important_bodies_path = '/nrs/flyem/bergs/complete-ffn-agglo/bodies-0.5-including-psds-from-neuprint-52f9.csv'\n",
    "important_bodies = pd.read_csv(important_bodies_path, header=0, usecols=['bodyid'], dtype=np.uint64)['bodyid']\n",
    "important_bodies = set(important_bodies)\n",
    "\n",
    "# Read last set of new bodies (from analysis node, after neo4j was loaded).\n",
    "msgs = read_kafka_messages(analysis_node, 'split', 'leaf-only')\n",
    "final_new_bodies = set(chain(*((msg['Target'], msg['NewLabel']) for msg in msgs)))\n",
    "\n",
    "# Append final set\n",
    "important_bodies |= final_new_bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_a, label_b -- the two supervoxel IDs\n",
    "# xa, ya, za -- point from which segmentation of 'a' was started, 8 nm coordinates\n",
    "# xb, yb, zb -- point from which segmentation of 'b' was started, 8 nm coordinates\n",
    "# caa, cab, cba, cbb -- cXY means: fraction of voxels from the original segment Y recovered when seeding from X\n",
    "# iou -- Jaccard index of the two local segmentations\n",
    "# da, db -- dX means: fraction of voxels that changed value from >0.8 to <0.5 when segmenting & seeding from X;\n",
    "#                     the higher this value is, the more \"internally inconsistent\" the segmentation resolution\n",
    "#                     potentially is; higher thresholds for iou, cXY might be warranted\n",
    "\n",
    "csv_dtypes = { 'id_a': np.uint64, 'id_b': np.uint64, # Use'id_a', and 'id_b' for consistency with our other code.\n",
    "               'xa': np.int32, 'ya': np.int32, 'za': np.int32,\n",
    "               'xb': np.int32, 'yb': np.int32, 'zb': np.int32,\n",
    "               'caa': np.float32, 'cab': np.float32, 'cba': np.float32, 'cbb': np.float32,\n",
    "               'iou': np.float32,\n",
    "               'da': np.float32, 'db': np.float32 }\n",
    "\n",
    "TOTAL_EDGE_COUNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 32nm files to npy...\n",
      "Converting 32nm files to npy took 0:00:05.185719\n",
      "Converting 16nm files to npy...\n",
      "Converting 16nm files to npy took 0:00:20.241233\n",
      "Converting 8nm files to npy...\n",
      "Converting 8nm files to npy took 0:00:38.211370\n"
     ]
    }
   ],
   "source": [
    "csv_paths = {}\n",
    "\n",
    "for res in [32, 16, 8]:\n",
    "    csv_paths[res] = [os.path.abspath(f'{res}nm/data-000{i:02d}-of-00100.csv')\n",
    "                          for i in range(100)]\n",
    "\n",
    "def save_as_npy(resolution, csv_path):\n",
    "    \"\"\"\n",
    "    Convert the given CSV edge table to .npy format,\n",
    "    and append a column for 'resolution' in the process.\n",
    "    \"\"\"\n",
    "    npy_path = os.path.splitext(csv_path)[0] + '.npy'\n",
    "    df = pd.read_csv(csv_path, header=None, names=list(csv_dtypes.keys()), dtype=csv_dtypes)\n",
    "    df['resolution'] = np.uint8(res)\n",
    "    np.save(npy_path, df.to_records(index=False))\n",
    "    return len(df)\n",
    "\n",
    "TOTAL_EDGE_COUNT = 0\n",
    "for res, paths in csv_paths.items():\n",
    "    with Timer(f\"Converting {res}nm files to npy\"):\n",
    "        counts = sc.parallelize(paths).map(lambda p: save_as_npy(res, p)).collect()\n",
    "        TOTAL_EDGE_COUNT += sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_npy_paths = (  sorted(glob.glob('32nm/*.npy'))\n",
    "                  + sorted(glob.glob('16nm/*.npy'))\n",
    "                  + sorted(glob.glob('8nm/*.npy')))\n",
    "orig_npy_paths = list(map(os.path.abspath, orig_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOTAL_EDGE_COUNT == 0:\n",
    "    def npy_len(path):\n",
    "        return len(np.load(path))\n",
    "    paths = map(os.path.abspath, glob.glob(f'*nm/*.npy'))\n",
    "    TOTAL_EDGE_COUNT = sc.parallelize(paths).map(npy_len).sum()\n",
    "print(f\"TOTAL_EDGE_COUNT: {TOTAL_EDGE_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GB: 113.233191336\n"
     ]
    }
   ],
   "source": [
    "first = np.load('32nm/data-00000-of-00100.npy')\n",
    "TABLE_DTYPE = first.dtype\n",
    "EDGE_NBYTES = first[0].nbytes\n",
    "TOTAL_NBYTES = EDGE_NBYTES * TOTAL_EDGE_COUNT\n",
    "del first\n",
    "print(f\"Total GB: {TOTAL_NBYTES / 1e9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%time combined_edge_table = np.fromiter(chain(*(np.load(p) for p in all_npy_files)), TABLE_DTYPE, TOTAL_EDGE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair coordinates for split supervoxels\n",
    "os.makedirs('split-coords-fixed/32nm', exist_ok=True)\n",
    "os.makedirs('split-coords-fixed/16nm', exist_ok=True)\n",
    "os.makedirs('split-coords-fixed/8nm', exist_ok=True)\n",
    "\n",
    "def repair_coords_on_splits(orig_npy_path):\n",
    "    \"\"\"\n",
    "    Read the given original npy path, repair coordinates for\n",
    "    edges mentioning anything in the retired_svs set,\n",
    "    and save the repaired file to a different directory.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.load(orig_npy_path))\n",
    "    print(\"Selecting retired supervoxels\")\n",
    "    retired_svs # Reference this variable to ensure that it gets captured when pickling this function.\n",
    "    rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "    print(f\"Found {rows_to_fix.sum()} rows\")\n",
    "    \n",
    "    fixed_points = []\n",
    "    df_to_fix = df[rows_to_fix]\n",
    "    for row in tqdm(df_to_fix.itertuples(), total=len(df_to_fix)):\n",
    "        new_points = None\n",
    "        coord_a = np.array((row.za, row.ya, row.xa))\n",
    "        coord_b = np.array((row.zb, row.yb, row.xb))\n",
    "        avg_coord = (coord_a + coord_b) // 2\n",
    "\n",
    "        for search_radius in [64, 128, 256]:\n",
    "            box = np.array(( avg_coord - search_radius,\n",
    "                             avg_coord + search_radius ))\n",
    "\n",
    "            sv_vol = fetch_labelarray_voxels(initial_agglo, box, supervoxels=True)\n",
    "            \n",
    "            # Try finding a touch point\n",
    "            touching_points = np.array(find_best_plane(sv_vol, row.id_a, row.id_b))\n",
    "            if not (touching_points == -1).all():\n",
    "                new_points = touching_points + box[0]\n",
    "                break\n",
    "            \n",
    "            # Try finding \"closest approach\" instead.\n",
    "            if (row.id_a in sv_vol.flat) and (row.id_b in sv_vol.flat):\n",
    "                # both ids are present in the volume,\n",
    "                # but they are not touching.\n",
    "                # Find the points that minimally separate them.\n",
    "                point_a, point_b = closest_approach(sv_vol, row.id_a, row.id_b)\n",
    "                new_points = np.array((point_a, point_b)) + box[0]\n",
    "                break\n",
    "\n",
    "\n",
    "        if new_points is None:\n",
    "            # The bodies are so far apart that we couldn't find a \"closst approach\"\n",
    "            # If the original points are at least on the correct supervoxels,\n",
    "            # settle for that.\n",
    "            if ( fetch_label_for_coordinate(initial_agglo, coord_a, True) == row.id_a\n",
    "             and fetch_label_for_coordinate(initial_agglo, coord_b, True) == row.id_b ):\n",
    "                new_points = np.array((coord_a, coord_b))\n",
    "            else:\n",
    "                # Couldn't find good points via any method at any radius.\n",
    "                # Indicate this by negating the coordinates.\n",
    "                new_points = np.array((-coord_a, -coord_b))\n",
    "\n",
    "        fixed_points.append( new_points )\n",
    "        \n",
    "    fixed_points = np.array(fixed_points)\n",
    "    df.loc[rows_to_fix, ['za', 'ya', 'xa']] = fixed_points[:,0,:]\n",
    "    df.loc[rows_to_fix, ['zb', 'yb', 'xb']] = fixed_points[:,1,:]\n",
    "    \n",
    "    parts = orig_npy_path.split('/')\n",
    "    parts.insert(-2, 'split-coords-fixed')\n",
    "    new_npy_path = '/'.join(parts)\n",
    "    np.save(new_npy_path, df.to_records(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.load('32nm/data-00000-of-00100.npy'))\n",
    "# print(\"Selecting retired supervoxels\")\n",
    "# rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "# print(f\"Found {rows_to_fix.sum()} rows\")\n",
    "# df_to_fix = df[rows_to_fix]\n",
    "# df_to_fix.iloc[37:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(543247105, 670798661, 31460, 23952, 9816, 31496, 24064, 9656,  0.94719797,  0.,  0.00039258,  0.99156862,  0.,  0.,  0.01129194, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = np.load('32nm/data-00000-of-00100.npy')\n",
    "first[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bf80e531384893866b331e76bdcf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loaded = []\n",
    "from tqdm import tqdm_notebook\n",
    "for i in tnrange(100):\n",
    "    loaded.append(np.load(f'split-coords-fixed/32nm/data-{i:05d}-of-00100.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70479347,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_32 = np.concatenate(loaded)\n",
    "fixed_32.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7393, 16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame(fixed_32)\n",
    "bad_rows = _df.query('xa < 0 or xb < 0')\n",
    "bad_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>xa</th>\n",
       "      <th>ya</th>\n",
       "      <th>za</th>\n",
       "      <th>xb</th>\n",
       "      <th>yb</th>\n",
       "      <th>zb</th>\n",
       "      <th>caa</th>\n",
       "      <th>cab</th>\n",
       "      <th>cba</th>\n",
       "      <th>cbb</th>\n",
       "      <th>iou</th>\n",
       "      <th>da</th>\n",
       "      <th>db</th>\n",
       "      <th>resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14684</th>\n",
       "      <td>1320291129</td>\n",
       "      <td>1976440135</td>\n",
       "      <td>-8540</td>\n",
       "      <td>-27388</td>\n",
       "      <td>-30056</td>\n",
       "      <td>-8596</td>\n",
       "      <td>-27448</td>\n",
       "      <td>-30072</td>\n",
       "      <td>0.951630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24715</th>\n",
       "      <td>1320291129</td>\n",
       "      <td>2444394053</td>\n",
       "      <td>-14320</td>\n",
       "      <td>-30200</td>\n",
       "      <td>-34716</td>\n",
       "      <td>-14304</td>\n",
       "      <td>-30348</td>\n",
       "      <td>-34776</td>\n",
       "      <td>0.998321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44002</th>\n",
       "      <td>481726717</td>\n",
       "      <td>451364234</td>\n",
       "      <td>-18976</td>\n",
       "      <td>-24408</td>\n",
       "      <td>-13444</td>\n",
       "      <td>-18936</td>\n",
       "      <td>-24484</td>\n",
       "      <td>-13352</td>\n",
       "      <td>0.973274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>0.975168</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49105</th>\n",
       "      <td>1320291129</td>\n",
       "      <td>1786159391</td>\n",
       "      <td>-12084</td>\n",
       "      <td>-22224</td>\n",
       "      <td>-25084</td>\n",
       "      <td>-12208</td>\n",
       "      <td>-22204</td>\n",
       "      <td>-25176</td>\n",
       "      <td>0.927857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.936408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94522</th>\n",
       "      <td>1320291129</td>\n",
       "      <td>2063764182</td>\n",
       "      <td>-11384</td>\n",
       "      <td>-20124</td>\n",
       "      <td>-29432</td>\n",
       "      <td>-11440</td>\n",
       "      <td>-19988</td>\n",
       "      <td>-29396</td>\n",
       "      <td>0.935989</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.963802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_a        id_b     xa     ya     za     xb     yb     zb  \\\n",
       "14684  1320291129  1976440135  -8540 -27388 -30056  -8596 -27448 -30072   \n",
       "24715  1320291129  2444394053 -14320 -30200 -34716 -14304 -30348 -34776   \n",
       "44002   481726717   451364234 -18976 -24408 -13444 -18936 -24484 -13352   \n",
       "49105  1320291129  1786159391 -12084 -22224 -25084 -12208 -22204 -25176   \n",
       "94522  1320291129  2063764182 -11384 -20124 -29432 -11440 -19988 -29396   \n",
       "\n",
       "            caa       cab       cba       cbb       iou        da        db  \\\n",
       "14684  0.951630  0.000000  0.000000  0.908958  0.000000  0.000101  0.000000   \n",
       "24715  0.998321  0.000000  0.000000  0.954720  0.000000  0.000000  0.006081   \n",
       "44002  0.973274  0.000000  0.004748  0.975168  0.000007  0.000116  0.000000   \n",
       "49105  0.927857  0.000000  0.005743  0.936408  0.000000  0.000100  0.033807   \n",
       "94522  0.935989  0.000488  0.000247  0.963802  0.000000  0.001051  0.000073   \n",
       "\n",
       "       resolution  \n",
       "14684          32  \n",
       "24715          32  \n",
       "44002          32  \n",
       "49105          32  \n",
       "94522          32  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1727 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting retired supervoxels\n",
      "Found 1727 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/flyem/proj/cluster/miniforge/envs/flyem/lib/python3.6/site-packages/numpy/linalg/linalg.py:2198: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "\n",
      "  0%|          | 1/1727 [00:00<14:34,  1.97it/s]\u001b[A\n",
      "  0%|          | 2/1727 [00:01<14:39,  1.96it/s]\u001b[A\n",
      "  0%|          | 3/1727 [00:01<14:31,  1.98it/s]\u001b[A\n",
      "  0%|          | 4/1727 [00:01<12:27,  2.31it/s]\u001b[A\n",
      "  0%|          | 5/1727 [00:01<11:20,  2.53it/s]\u001b[A\n",
      "  0%|          | 6/1727 [00:02<10:35,  2.71it/s]\u001b[A\n",
      "  0%|          | 7/1727 [00:02<09:44,  2.94it/s]\u001b[A\n",
      "  0%|          | 8/1727 [00:02<10:23,  2.76it/s]\u001b[A\n",
      "  1%|          | 9/1727 [00:03<10:04,  2.84it/s]\u001b[A\n",
      "  1%|          | 10/1727 [00:03<09:41,  2.95it/s]\u001b[A\n",
      "  1%|          | 11/1727 [00:03<09:19,  3.07it/s]\u001b[A\n",
      "  1%|          | 12/1727 [00:04<09:45,  2.93it/s]\u001b[A\n",
      "  1%|          | 13/1727 [00:04<10:19,  2.77it/s]\u001b[A\n",
      "  1%|          | 14/1727 [00:04<10:08,  2.81it/s]\u001b[A\n",
      "  1%|          | 15/1727 [00:05<09:53,  2.88it/s]\u001b[A\n",
      "  1%|          | 16/1727 [00:05<09:41,  2.94it/s]\u001b[A\n",
      "  1%|          | 17/1727 [00:05<09:49,  2.90it/s]\u001b[A\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/groups/flyem/proj/cluster/miniforge/envs/flyem/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/groups/flyem/proj/cluster/miniforge/envs/flyem/lib/python3.6/site-packages/tqdm/_monitor.py\", line 63, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/groups/flyem/proj/cluster/miniforge/envs/flyem/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 1727/1727 [43:15<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "repair_coords_on_splits('32nm/data-00000-of-00100.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b0fc26f9362a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_npy_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepair_coords_on_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.parallelize(orig_npy_paths).foreach(repair_coords_on_splits)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_npy_paths = (  sorted(glob.glob('split-coords-fixed/32nm/*.npy'))\n",
    "                   + sorted(glob.glob('split-coords-fixed/16nm/*.npy'))\n",
    "                   + sorted(glob.glob('split-coords-fixed/8nm/*.npy')))\n",
    "fixed_npy_paths = list(map(os.path.abspath, fixed_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 10.6 ms, total: 36.7 ms\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def count_unfixable(npy_path):\n",
    "    return (np.load(npy_path)['za'] < 0).sum()\n",
    "unfixable_count = sc.parallelize(fixed_npy_paths).map(count_unfixable).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel table SVs from init agglo to current master\n",
    "(and drop bad edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('updated-tables/32nm', exist_ok=True)\n",
    "os.makedirs('updated-tables/16nm', exist_ok=True)\n",
    "os.makedirs('updated-tables/8nm', exist_ok=True)\n",
    "\n",
    "# Replace old SV ids with updated IDs by sampling from those coordinates.\n",
    "def remap_split_svs(npy_path):\n",
    "    df = pd.DataFrame(np.load(npy_path))\n",
    "    assert df['id_a'].dtype == np.uint64\n",
    "    assert df['id_b'].dtype == np.uint64\n",
    "\n",
    "    retired_svs # Reference this variable to ensure that it gets captured when pickling this function.\n",
    "    rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "\n",
    "    fixed_ids = []\n",
    "    df_to_fix = df[rows_to_fix]\n",
    "    for row in tqdm(df_to_fix.itertuples(), total=len(df_to_fix)):\n",
    "        id_a, id_b = row.id_a, row.id_b\n",
    "        if id_a in retired_svs:\n",
    "            id_a = fetch_label_for_coordinate(analysis_node, (row.za, row.ya, row.xa), supervoxels=True)\n",
    "        if id_b in retired_svs:\n",
    "            id_b = fetch_label_for_coordinate(analysis_node, (row.zb, row.yb, row.xb), supervoxels=True)\n",
    "        fixed_ids.append( (id_a, id_b) )\n",
    "\n",
    "    df.loc[rows_to_fix, ['id_a', 'id_b']] = np.array(fixed_ids, np.uint64)\n",
    "    assert df['id_a'].dtype == np.uint64\n",
    "    assert df['id_b'].dtype == np.uint64\n",
    "\n",
    "    parts = npy_path.split('/')\n",
    "    assert parts[-3] == 'split-coords-fixed'\n",
    "    parts[-3] = 'updated-tables'\n",
    "    new_npy_path = '/'.join(parts)\n",
    "    np.save(new_npy_path, df.to_records(index=False))\n",
    "\n",
    "    return rows_to_fix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap_split_svs(fixed_npy_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 204 ms, sys: 46.3 ms, total: 250 ms\n",
      "Wall time: 13min 29s\n"
     ]
    }
   ],
   "source": [
    "%time updated_row_count = sc.parallelize(fixed_npy_paths).map(remap_split_svs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081409"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = fetch_mappings(analysis_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 471 ms, total: 24.4 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%time mapper = LabelMapper(mapping.index.values, mapping.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('filtered-tables/32nm', exist_ok=True)\n",
    "os.makedirs('filtered-tables/16nm', exist_ok=True)\n",
    "os.makedirs('filtered-tables/8nm', exist_ok=True)\n",
    "\n",
    "# Replace old SV ids with updated IDs by sampling from those coordinates.\n",
    "def apply_mapping_and_filter_to_partition(paths):\n",
    "    # Must create mapper here since it cannot be pickled.\n",
    "    mapper = LabelMapper(mapping.index.values, mapping.values)\n",
    "\n",
    "    def apply_mapping_and_filter(npy_path):\n",
    "        df = pd.DataFrame(np.load(npy_path))\n",
    "\n",
    "        # A bug above caused the type to be int64. Fix that now.\n",
    "        df['id_a'] = df['id_a'].astype(np.uint64)\n",
    "        df['id_b'] = df['id_b'].astype(np.uint64)\n",
    "        \n",
    "        df['body_a'] = mapper.apply(df['id_a'].values, allow_unmapped=True)\n",
    "        df['body_b'] = mapper.apply(df['id_b'].values, allow_unmapped=True)\n",
    "\n",
    "        important_bodies # Referenced to ensure capture in this closure\n",
    "\n",
    "        # Drop internal edges,\n",
    "        # Filter for important bodies (on at least one end -- capture 1-hop and 2-hop)\n",
    "        q = '(body_a != body_b) and ((body_a in @important_bodies) or (body_b in @important_bodies))'\n",
    "        df.query(q, inplace=True)\n",
    "\n",
    "        parts = npy_path.split('/')\n",
    "        assert parts[-3] == 'updated-tables'\n",
    "        parts[-3] = 'filtered-tables'\n",
    "        new_npy_path = '/'.join(parts)\n",
    "        np.save(new_npy_path, df.to_records(index=False))\n",
    "\n",
    "        return len(df)\n",
    "    \n",
    "    return list(map(apply_mapping_and_filter, paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_npy_paths = (  sorted(glob.glob('updated-tables/32nm/*.npy'))\n",
    "                     + sorted(glob.glob('updated-tables/16nm/*.npy'))\n",
    "                     + sorted(glob.glob('updated-tables/8nm/*.npy')))\n",
    "updated_npy_paths = list(map(os.path.abspath, updated_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 4.18 s, total: 27.7 s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "filtered_row_count = (sc.parallelize(updated_npy_paths)\n",
    "                        .mapPartitions(apply_mapping_and_filter_to_partition)\n",
    "                        .sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755536110\n"
     ]
    }
   ],
   "source": [
    "print(filtered_row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_npy_paths = (  sorted(glob.glob('filtered-tables/32nm/*.npy'))\n",
    "                      + sorted(glob.glob('filtered-tables/16nm/*.npy'))\n",
    "                      + sorted(glob.glob('filtered-tables/8nm/*.npy')))\n",
    "filtered_npy_paths = list(map(os.path.abspath, filtered_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:14<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_table = np.concatenate(list(map(np.load, tqdm(filtered_npy_paths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755.53611 M\n",
      "64.22056935 GB\n"
     ]
    }
   ],
   "source": [
    "print(combined_table.shape[0] / 1e6, \"M\")\n",
    "print(combined_table.nbytes / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame(combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 s, sys: 1min 7s, total: 1min 10s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%time np.save('combined-filtered-table.npy', combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27m16nm\u001b[0m/                               \u001b[38;5;27mnotebook-cluster--20180723.094528\u001b[0m/\r\n",
      "\u001b[38;5;27m32nm\u001b[0m/                               \u001b[38;5;27mnotebook-cluster--20180723.094651\u001b[0m/\r\n",
      "\u001b[38;5;27m8nm\u001b[0m/                                \u001b[38;5;27mnotebook-cluster--20180723.180735\u001b[0m/\r\n",
      "bodies-0.5-from-neuprint-52f9.csv   spark-focused.ipynb\r\n",
      "\u001b[38;5;27mfiltered-tables\u001b[0m/                    \u001b[38;5;27msplit-coords-fixed\u001b[0m/\r\n",
      "\u001b[38;5;27mnotebook-cluster--20180722.201030\u001b[0m/  \u001b[38;5;27mupdated-tables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
