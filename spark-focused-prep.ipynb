{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from itertools import chain\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "#tqdm = tqdm_notebook\n",
    "\n",
    "import vigra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dvidutils import LabelMapper\n",
    "from libdvid import DVIDNodeService\n",
    "\n",
    "from neuclease.dvid import *\n",
    "from neuclease.util import Timer\n",
    "from neuclease.misc import find_best_plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DVIDSparkServices.spark_launch_scripts.janelia_lsf.lsf_utils import get_hostgraph_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler(sys.stdout)\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.handlers = []\n",
    "root_logger.addHandler(handler)\n",
    "root_logger.setLevel(logging.INFO)\n",
    "logging.getLogger('kafka').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nrs/flyem/bergs/complete-ffn-agglo\n"
     ]
    }
   ],
   "source": [
    "cd /nrs/flyem/bergs/complete-ffn-agglo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h10u12.int.janelia.org\r\n"
     ]
    }
   ],
   "source": [
    "!uname -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nrs/flyem/bergs/complete-ffn-agglo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://h10u12:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://h02u03:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://h02u03:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_approach(sv_vol, id_a, id_b):\n",
    "    \"\"\"\n",
    "    Given a segmentation volume and two label IDs which it contains,\n",
    "    Find the two coordinates within id_a and id_b, respectively,\n",
    "    which mark the two objects' closest approach, i.e. where the objects\n",
    "    come closest to touching, even if they don't actually touch.\n",
    "    \n",
    "    Returns (coord_a, coord_b)\n",
    "    \"\"\"\n",
    "    # For all voxels, find the shortest vector toward id_b\n",
    "    to_b_vectors = vigra.filters.vectorDistanceTransform((sv_vol == id_b).astype(np.uint32))\n",
    "\n",
    "    # Magnitude of those vectors == distance to id_b\n",
    "    to_b_distances = np.linalg.norm(to_b_vectors, axis=-1)\n",
    "\n",
    "    # We're only interested in the voxels within id_a;\n",
    "    # everything else is infinite distance\n",
    "    to_b_distances[sv_vol != id_a] = np.inf\n",
    "\n",
    "    # Find the point within id_a with the smallest vector\n",
    "    point_a = np.unravel_index(np.argmin(to_b_distances), to_b_distances.shape)\n",
    "\n",
    "    # Its closest point id_b is indicated by the corresponding vector\n",
    "    point_b = (point_a + to_b_vectors[point_a]).astype(int)\n",
    "\n",
    "    return (point_a, point_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_events_to_mapping(split_events, leaves_only=False):\n",
    "    \"\"\"\n",
    "    Convert the given split_events,\n",
    "    into a mapping, from all split fragment supervoxel IDs to their ROOT supervoxel ID,\n",
    "    i.e. the supervoxel from which they came originally.\n",
    "\n",
    "    Args:\n",
    "        split_events:\n",
    "            As produced by fetch_supervoxel_splits()\n",
    "\n",
    "        leaves_only:\n",
    "            If True, do not include intermediate supervoxels in the mapping;\n",
    "            only include fragment IDs that have not been further split,\n",
    "            i.e. they still exist in the volume.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series, where index is fragment ID, data is root ID.\n",
    "    \"\"\"\n",
    "    if len(split_events) == 0:\n",
    "        return np.zeros((0,2), np.uint64)\n",
    "    \n",
    "    split_tables = list(map(lambda t: np.asarray(t, np.uint64), split_events.values()))\n",
    "    split_table = np.concatenate(split_tables)\n",
    "\n",
    "    old_svs = split_table[:, SplitEvent._fields.index('old')]\n",
    "    remain_fragment_svs = split_table[:, SplitEvent._fields.index('remain')]\n",
    "    split_fragment_svs = split_table[:, SplitEvent._fields.index('split')]\n",
    "\n",
    "    if leaves_only:\n",
    "        leaf_fragment_svs = (set(remain_fragment_svs) | set(split_fragment_svs)) - set(old_svs)\n",
    "        fragment_svs = np.fromiter(leaf_fragment_svs, np.uint64)\n",
    "    else:\n",
    "        fragment_svs = np.concatenate((remain_fragment_svs, split_fragment_svs))\n",
    "        \n",
    "    g = split_events_to_graph(split_events)\n",
    "    root_svs = np.fromiter(map(lambda sv: find_root(g, sv), fragment_svs), np.uint64, len(fragment_svs))\n",
    "\n",
    "    mapping = pd.Series(index=fragment_svs, data=root_svs)\n",
    "    mapping.index.name = 'fragment_sv'\n",
    "    mapping.name = 'root_sv'\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sanitize_server\n",
    "def expand_uuid(server, uuid, repo_uuid=None):\n",
    "    repo_uuid = repo_uuid or uuid\n",
    "    repo_info = fetch_repo_info(server, repo_uuid)\n",
    "    full_uuids = repo_info[\"DAG\"][\"Nodes\"].keys()\n",
    "    \n",
    "    matching_uuids = list(filter(lambda full_uuid: uuids_match(uuid, full_uuid), full_uuids))\n",
    "    if len(matching_uuids) == 0:\n",
    "        raise RuntimeError(f\"No matching uuid for '{uuid}'\")\n",
    "    \n",
    "    if len(matching_uuids) > 1:\n",
    "        raise RuntimeError(f\"Multiple ({len(matching_uuids)}) uuids match '{uuid}': {matching_uuids}\")\n",
    "\n",
    "    return matching_uuids[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hostgraph URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook:\n",
      "http://lsf-rtm/cacti/plugins/grid/grid_bjobs.php?action=viewjob&tab=hostgraph&clusterid=1&indexid=0&jobid=44106114&submit_time=1532486124\n",
      "Cluster:\n",
      "http://lsf-rtm/cacti/plugins/grid/grid_bjobs.php?action=viewjob&tab=hostgraph&clusterid=1&indexid=0&jobid=44106116&submit_time=1532486137\n"
     ]
    }
   ],
   "source": [
    "print(\"This notebook:\")\n",
    "print(get_hostgraph_url(os.environ[\"LSB_JOBID\"]))\n",
    "print(\"Cluster:\")\n",
    "print(get_hostgraph_url(os.environ[\"MASTER_BJOB_ID\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting agglo\n",
    "initial_agglo = DvidInstanceInfo('emdata3:8900', 'ac901', 'segmentation')\n",
    "\n",
    "# The uuid used when loading the neo4j instance (for 'important bodies')\n",
    "neo4j_reference = DvidInstanceInfo('emdata3:8900', '52f9', 'segmentation')\n",
    "\n",
    "# The last supervoxel splits: One past the neo4j node\n",
    "analysis_node = DvidInstanceInfo('emdata3:8900', '662e', 'segmentation')\n",
    "\n",
    "# We won't be using this...\n",
    "current_master = DvidInstanceInfo('emdata3:8900', 'f545', 'segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load split SVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166737 kafka messages took 7.26911473274231 seconds\n",
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166737 kafka messages took 6.93674635887146 seconds\n"
     ]
    }
   ],
   "source": [
    "leaf_fragment_svs, retired_svs = fetch_supervoxel_fragments(analysis_node, 'kafka')\n",
    "retired_svs = set(retired_svs)\n",
    "split_events = fetch_supervoxel_splits(analysis_node, 'kafka')\n",
    "split_mapping = split_events_to_mapping(split_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load neo4j-defined important bodies; append final splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166737 kafka messages took 6.982581615447998 seconds\n"
     ]
    }
   ],
   "source": [
    "# This list was generated from node 52f9\n",
    "important_bodies_path = '/nrs/flyem/bergs/complete-ffn-agglo/bodies-0.5-including-psds-from-neuprint-52f9.csv'\n",
    "important_bodies = pd.read_csv(important_bodies_path, header=0, usecols=['bodyid'], dtype=np.uint64)['bodyid']\n",
    "important_bodies = set(important_bodies)\n",
    "\n",
    "# Read last set of new bodies (from analysis node, after neo4j was loaded).\n",
    "msgs = read_kafka_messages(analysis_node, 'split', 'leaf-only')\n",
    "final_new_bodies = set(chain(*((msg['Target'], msg['NewLabel']) for msg in msgs)))\n",
    "\n",
    "# Append final set\n",
    "important_bodies |= final_new_bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_a, label_b -- the two supervoxel IDs\n",
    "# xa, ya, za -- point from which segmentation of 'a' was started, 8 nm coordinates\n",
    "# xb, yb, zb -- point from which segmentation of 'b' was started, 8 nm coordinates\n",
    "# caa, cab, cba, cbb -- cXY means: fraction of voxels from the original segment Y recovered when seeding from X\n",
    "# iou -- Jaccard index of the two local segmentations\n",
    "# da, db -- dX means: fraction of voxels that changed value from >0.8 to <0.5 when segmenting & seeding from X;\n",
    "#                     the higher this value is, the more \"internally inconsistent\" the segmentation resolution\n",
    "#                     potentially is; higher thresholds for iou, cXY might be warranted\n",
    "\n",
    "csv_dtypes = { 'id_a': np.uint64, 'id_b': np.uint64, # Use'id_a', and 'id_b' for consistency with our other code.\n",
    "               'xa': np.int32, 'ya': np.int32, 'za': np.int32,\n",
    "               'xb': np.int32, 'yb': np.int32, 'zb': np.int32,\n",
    "               'caa': np.float32, 'cab': np.float32, 'cba': np.float32, 'cbb': np.float32,\n",
    "               'iou': np.float32,\n",
    "               'da': np.float32, 'db': np.float32 }\n",
    "\n",
    "TOTAL_EDGE_COUNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 32nm files to npy...\n",
      "Converting 32nm files to npy took 0:00:05.185719\n",
      "Converting 16nm files to npy...\n",
      "Converting 16nm files to npy took 0:00:20.241233\n",
      "Converting 8nm files to npy...\n",
      "Converting 8nm files to npy took 0:00:38.211370\n"
     ]
    }
   ],
   "source": [
    "csv_paths = {}\n",
    "\n",
    "for res in [32, 16, 8]:\n",
    "    csv_paths[res] = [os.path.abspath(f'{res}nm/data-000{i:02d}-of-00100.csv')\n",
    "                          for i in range(100)]\n",
    "\n",
    "def save_as_npy(resolution, csv_path):\n",
    "    \"\"\"\n",
    "    Convert the given CSV edge table to .npy format,\n",
    "    and append a column for 'resolution' in the process.\n",
    "    \"\"\"\n",
    "    npy_path = os.path.splitext(csv_path)[0] + '.npy'\n",
    "    df = pd.read_csv(csv_path, header=None, names=list(csv_dtypes.keys()), dtype=csv_dtypes)\n",
    "    df['resolution'] = np.uint8(res)\n",
    "    np.save(npy_path, df.to_records(index=False))\n",
    "    return len(df)\n",
    "\n",
    "TOTAL_EDGE_COUNT = 0\n",
    "for res, paths in csv_paths.items():\n",
    "    with Timer(f\"Converting {res}nm files to npy\"):\n",
    "        counts = sc.parallelize(paths).map(lambda p: save_as_npy(res, p)).collect()\n",
    "        TOTAL_EDGE_COUNT += sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_npy_paths = (  sorted(glob.glob('32nm/*.npy'))\n",
    "                  + sorted(glob.glob('16nm/*.npy'))\n",
    "                  + sorted(glob.glob('8nm/*.npy')))\n",
    "orig_npy_paths = list(map(os.path.abspath, orig_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cba34638d1f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'*nm/*.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mTOTAL_EDGE_COUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"TOTAL_EDGE_COUNT: {TOTAL_EDGE_COUNT}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/misc/local/spark-test/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/misc/local/spark-test/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/misc/local/spark-test/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/misc/local/spark-test/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/misc/local/spark-test/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/misc/local/spark-test/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/groups/flyem/proj/cluster/miniforge/envs/flyem/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TOTAL_EDGE_COUNT == 0:\n",
    "    def npy_len(path):\n",
    "        return len(np.load(path))\n",
    "    paths = map(os.path.abspath, glob.glob(f'*nm/*.npy'))\n",
    "    TOTAL_EDGE_COUNT = sc.parallelize(paths).map(npy_len).sum()\n",
    "print(f\"TOTAL_EDGE_COUNT: {TOTAL_EDGE_COUNT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GB: 113.233191336\n"
     ]
    }
   ],
   "source": [
    "first = np.load('32nm/data-00000-of-00100.npy')\n",
    "TABLE_DTYPE = first.dtype\n",
    "EDGE_NBYTES = first[0].nbytes\n",
    "TOTAL_NBYTES = EDGE_NBYTES * TOTAL_EDGE_COUNT\n",
    "del first\n",
    "print(f\"Total GB: {TOTAL_NBYTES / 1e9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%time combined_edge_table = np.fromiter(chain(*(np.load(p) for p in all_npy_files)), TABLE_DTYPE, TOTAL_EDGE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair coordinates for split supervoxels\n",
    "os.makedirs('split-coords-fixed/32nm', exist_ok=True)\n",
    "os.makedirs('split-coords-fixed/16nm', exist_ok=True)\n",
    "os.makedirs('split-coords-fixed/8nm', exist_ok=True)\n",
    "\n",
    "def repair_coords_on_splits(orig_npy_path):\n",
    "    \"\"\"\n",
    "    Read the given original npy path, repair coordinates for\n",
    "    edges mentioning anything in the retired_svs set,\n",
    "    and save the repaired file to a different directory.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.load(orig_npy_path))\n",
    "    print(\"Selecting retired supervoxels\")\n",
    "    retired_svs # Reference this variable to ensure that it gets captured when pickling this function.\n",
    "    rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "    print(f\"Found {rows_to_fix.sum()} rows\")\n",
    "    \n",
    "    fixed_points = []\n",
    "    df_to_fix = df[rows_to_fix]\n",
    "    for row in tqdm(df_to_fix.itertuples(), total=len(df_to_fix)):\n",
    "        new_points = None\n",
    "        coord_a = np.array((row.za, row.ya, row.xa))\n",
    "        coord_b = np.array((row.zb, row.yb, row.xb))\n",
    "        avg_coord = (coord_a + coord_b) // 2\n",
    "\n",
    "        for search_radius in [64, 128, 256]:\n",
    "            box = np.array(( avg_coord - search_radius,\n",
    "                             avg_coord + search_radius ))\n",
    "\n",
    "            sv_vol = fetch_labelarray_voxels(initial_agglo, box, supervoxels=True)\n",
    "            \n",
    "            # Try finding a touch point\n",
    "            touching_points = np.array(find_best_plane(sv_vol, row.id_a, row.id_b))\n",
    "            if not (touching_points == -1).all():\n",
    "                new_points = touching_points + box[0]\n",
    "                break\n",
    "            \n",
    "            # Try finding \"closest approach\" instead.\n",
    "            if (row.id_a in sv_vol.flat) and (row.id_b in sv_vol.flat):\n",
    "                # both ids are present in the volume,\n",
    "                # but they are not touching.\n",
    "                # Find the points that minimally separate them.\n",
    "                point_a, point_b = closest_approach(sv_vol, row.id_a, row.id_b)\n",
    "                new_points = np.array((point_a, point_b)) + box[0]\n",
    "                break\n",
    "\n",
    "\n",
    "        if new_points is None:\n",
    "            # The bodies are so far apart that we couldn't find a \"closst approach\"\n",
    "            # If the original points are at least on the correct supervoxels,\n",
    "            # settle for that.\n",
    "            if ( fetch_label_for_coordinate(initial_agglo, coord_a, True) == row.id_a\n",
    "             and fetch_label_for_coordinate(initial_agglo, coord_b, True) == row.id_b ):\n",
    "                new_points = np.array((coord_a, coord_b))\n",
    "            else:\n",
    "                # Couldn't find good points via any method at any radius.\n",
    "                # Indicate this by negating the coordinates.\n",
    "                new_points = np.array((-coord_a, -coord_b))\n",
    "\n",
    "        fixed_points.append( new_points )\n",
    "        \n",
    "    fixed_points = np.array(fixed_points)\n",
    "    df.loc[rows_to_fix, ['za', 'ya', 'xa']] = fixed_points[:,0,:]\n",
    "    df.loc[rows_to_fix, ['zb', 'yb', 'xb']] = fixed_points[:,1,:]\n",
    "    \n",
    "    parts = orig_npy_path.split('/')\n",
    "    parts.insert(-2, 'split-coords-fixed')\n",
    "    new_npy_path = '/'.join(parts)\n",
    "    np.save(new_npy_path, df.to_records(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.load('32nm/data-00000-of-00100.npy'))\n",
    "# print(\"Selecting retired supervoxels\")\n",
    "# rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "# print(f\"Found {rows_to_fix.sum()} rows\")\n",
    "# df_to_fix = df[rows_to_fix]\n",
    "# df_to_fix.iloc[37:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repair_coords_on_splits('32nm/data-00000-of-00100.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize(orig_npy_paths).foreach(repair_coords_on_splits)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_npy_paths = (  sorted(glob.glob('split-coords-fixed/32nm/*.npy'))\n",
    "                   + sorted(glob.glob('split-coords-fixed/16nm/*.npy'))\n",
    "                   + sorted(glob.glob('split-coords-fixed/8nm/*.npy')))\n",
    "fixed_npy_paths = list(map(os.path.abspath, fixed_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 10.6 ms, total: 36.7 ms\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def count_unfixable(npy_path):\n",
    "    return (np.load(npy_path)['za'] < 0).sum()\n",
    "unfixable_count = sc.parallelize(fixed_npy_paths).map(count_unfixable).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel table SVs from init agglo to current master\n",
    "(and drop bad edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('updated-tables/32nm', exist_ok=True)\n",
    "os.makedirs('updated-tables/16nm', exist_ok=True)\n",
    "os.makedirs('updated-tables/8nm', exist_ok=True)\n",
    "\n",
    "# Replace old SV ids with updated IDs by sampling from those coordinates.\n",
    "def remap_split_svs(npy_path):\n",
    "    df = pd.DataFrame(np.load(npy_path))\n",
    "    assert df['id_a'].dtype == np.uint64\n",
    "    assert df['id_b'].dtype == np.uint64\n",
    "\n",
    "    retired_svs # Reference this variable to ensure that it gets captured when pickling this function.\n",
    "    rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "\n",
    "    fixed_ids = []\n",
    "    df_to_fix = df[rows_to_fix]\n",
    "    for row in tqdm(df_to_fix.itertuples(), total=len(df_to_fix)):\n",
    "        id_a, id_b = row.id_a, row.id_b\n",
    "        if id_a in retired_svs:\n",
    "            id_a = fetch_label_for_coordinate(analysis_node, (row.za, row.ya, row.xa), supervoxels=True)\n",
    "        if id_b in retired_svs:\n",
    "            id_b = fetch_label_for_coordinate(analysis_node, (row.zb, row.yb, row.xb), supervoxels=True)\n",
    "        fixed_ids.append( (id_a, id_b) )\n",
    "\n",
    "    df.loc[rows_to_fix, ['id_a', 'id_b']] = np.array(fixed_ids, np.uint64)\n",
    "    assert df['id_a'].dtype == np.uint64\n",
    "    assert df['id_b'].dtype == np.uint64\n",
    "\n",
    "    parts = npy_path.split('/')\n",
    "    assert parts[-3] == 'split-coords-fixed'\n",
    "    parts[-3] = 'updated-tables'\n",
    "    new_npy_path = '/'.join(parts)\n",
    "    np.save(new_npy_path, df.to_records(index=False))\n",
    "\n",
    "    return rows_to_fix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap_split_svs(fixed_npy_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 204 ms, sys: 46.3 ms, total: 250 ms\n",
      "Wall time: 13min 29s\n"
     ]
    }
   ],
   "source": [
    "%time updated_row_count = sc.parallelize(fixed_npy_paths).map(remap_split_svs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081409"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = fetch_mappings(analysis_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 471 ms, total: 24.4 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%time mapper = LabelMapper(mapping.index.values, mapping.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('filtered-tables/32nm', exist_ok=True)\n",
    "os.makedirs('filtered-tables/16nm', exist_ok=True)\n",
    "os.makedirs('filtered-tables/8nm', exist_ok=True)\n",
    "\n",
    "# Replace old SV ids with updated IDs by sampling from those coordinates.\n",
    "def apply_mapping_and_filter_to_partition(paths):\n",
    "    # Must create mapper here since it cannot be pickled.\n",
    "    mapper = LabelMapper(mapping.index.values, mapping.values)\n",
    "\n",
    "    def apply_mapping_and_filter(npy_path):\n",
    "        df = pd.DataFrame(np.load(npy_path))\n",
    "\n",
    "        # A bug above caused the type to be int64. Fix that now.\n",
    "        df['id_a'] = df['id_a'].astype(np.uint64)\n",
    "        df['id_b'] = df['id_b'].astype(np.uint64)\n",
    "        \n",
    "        df['body_a'] = mapper.apply(df['id_a'].values, allow_unmapped=True)\n",
    "        df['body_b'] = mapper.apply(df['id_b'].values, allow_unmapped=True)\n",
    "\n",
    "        important_bodies # Referenced to ensure capture in this closure\n",
    "\n",
    "        # Drop internal edges,\n",
    "        # Filter for important bodies (on at least one end -- capture 1-hop and 2-hop)\n",
    "        q = '(body_a != body_b) and ((body_a in @important_bodies) or (body_b in @important_bodies))'\n",
    "        df.query(q, inplace=True)\n",
    "\n",
    "        parts = npy_path.split('/')\n",
    "        assert parts[-3] == 'updated-tables'\n",
    "        parts[-3] = 'filtered-tables'\n",
    "        new_npy_path = '/'.join(parts)\n",
    "        np.save(new_npy_path, df.to_records(index=False))\n",
    "\n",
    "        return len(df)\n",
    "    \n",
    "    return list(map(apply_mapping_and_filter, paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_npy_paths = (  sorted(glob.glob('updated-tables/32nm/*.npy'))\n",
    "                     + sorted(glob.glob('updated-tables/16nm/*.npy'))\n",
    "                     + sorted(glob.glob('updated-tables/8nm/*.npy')))\n",
    "updated_npy_paths = list(map(os.path.abspath, updated_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 4.18 s, total: 27.7 s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "filtered_row_count = (sc.parallelize(updated_npy_paths)\n",
    "                        .mapPartitions(apply_mapping_and_filter_to_partition)\n",
    "                        .sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755536110\n"
     ]
    }
   ],
   "source": [
    "print(filtered_row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_npy_paths = (  sorted(glob.glob('filtered-tables/32nm/*.npy'))\n",
    "                      + sorted(glob.glob('filtered-tables/16nm/*.npy'))\n",
    "                      + sorted(glob.glob('filtered-tables/8nm/*.npy')))\n",
    "filtered_npy_paths = list(map(os.path.abspath, filtered_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:14<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_table = np.concatenate(list(map(np.load, tqdm(filtered_npy_paths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755.53611 M\n",
      "64.22056935 GB\n"
     ]
    }
   ],
   "source": [
    "print(combined_table.shape[0] / 1e6, \"M\")\n",
    "print(combined_table.nbytes / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame(combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 s, sys: 1min 7s, total: 1min 10s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%time np.save('combined-filtered-table.npy', combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27m16nm\u001b[0m/                               \u001b[38;5;27mnotebook-cluster--20180723.094528\u001b[0m/\r\n",
      "\u001b[38;5;27m32nm\u001b[0m/                               \u001b[38;5;27mnotebook-cluster--20180723.094651\u001b[0m/\r\n",
      "\u001b[38;5;27m8nm\u001b[0m/                                \u001b[38;5;27mnotebook-cluster--20180723.180735\u001b[0m/\r\n",
      "bodies-0.5-from-neuprint-52f9.csv   spark-focused.ipynb\r\n",
      "\u001b[38;5;27mfiltered-tables\u001b[0m/                    \u001b[38;5;27msplit-coords-fixed\u001b[0m/\r\n",
      "\u001b[38;5;27mnotebook-cluster--20180722.201030\u001b[0m/  \u001b[38;5;27mupdated-tables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
