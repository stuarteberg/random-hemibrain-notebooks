{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from itertools import chain\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "#tqdm = tqdm_notebook\n",
    "\n",
    "import vigra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dvidutils import LabelMapper\n",
    "from libdvid import DVIDNodeService\n",
    "\n",
    "from neuclease.dvid import *\n",
    "from neuclease.util import Timer\n",
    "from neuclease.misc import find_best_plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DVIDSparkServices.spark_launch_scripts.janelia_lsf.lsf_utils import get_hostgraph_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler(sys.stdout)\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.handlers = []\n",
    "root_logger.addHandler(handler)\n",
    "root_logger.setLevel(logging.INFO)\n",
    "logging.getLogger('kafka').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nrs/flyem/bergs/complete-ffn-agglo\n"
     ]
    }
   ],
   "source": [
    "cd /nrs/flyem/bergs/complete-ffn-agglo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h11u11.int.janelia.org\r\n"
     ]
    }
   ],
   "source": [
    "!uname -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nrs/flyem/bergs/complete-ffn-agglo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://h11u11:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://h02u02:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://h02u02:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hostgraph URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook:\n",
      "http://lsf-rtm/cacti/plugins/grid/grid_bjobs.php?action=viewjob&tab=hostgraph&clusterid=1&indexid=0&jobid=44073279&submit_time=1532443822\n",
      "Cluster:\n",
      "http://lsf-rtm/cacti/plugins/grid/grid_bjobs.php?action=viewjob&tab=hostgraph&clusterid=1&indexid=0&jobid=44073287&submit_time=1532443858\n"
     ]
    }
   ],
   "source": [
    "print(\"This notebook:\")\n",
    "print(get_hostgraph_url(os.environ[\"LSB_JOBID\"]))\n",
    "print(\"Cluster:\")\n",
    "print(get_hostgraph_url(os.environ[\"MASTER_BJOB_ID\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting agglo\n",
    "initial_agglo = DvidInstanceInfo('emdata3:8900', 'ac901', 'segmentation')\n",
    "\n",
    "# The uuid used when loading the neo4j instance (for 'important bodies')\n",
    "neo4j_reference = DvidInstanceInfo('emdata3:8900', '52f9', 'segmentation')\n",
    "\n",
    "# The last supervoxel splits: One past the neo4j node\n",
    "analysis_node = DvidInstanceInfo('emdata3:8900', '662e', 'segmentation')\n",
    "\n",
    "# We won't be using this...\n",
    "current_master = DvidInstanceInfo('emdata3:8900', 'f545', 'segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load neo4j-defined important bodies; append final splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading kafka messages from kafka.int.janelia.org:9092 for emdata3:8900 / 662e / segmentation\n",
      "Reading 166499 kafka messages took 6.998676776885986 seconds\n"
     ]
    }
   ],
   "source": [
    "# This list was generated from node 52f9\n",
    "important_bodies_path = '/nrs/flyem/bergs/complete-ffn-agglo/bodies-0.5-from-neuprint-52f9.csv'\n",
    "important_bodies = pd.read_csv(important_bodies_path, header=0, usecols=['bodyId'], dtype=np.uint64)['bodyId']\n",
    "important_bodies = set(important_bodies)\n",
    "\n",
    "# Read last set of new bodies (from analysis node, after neo4j was loaded).\n",
    "msgs = read_kafka_messages(analysis_node, 'split', 'leaf-only')\n",
    "final_new_bodies = set(chain(*((msg['Target'], msg['NewLabel']) for msg in msgs)))\n",
    "\n",
    "# Append final set\n",
    "important_bodies |= final_new_bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-filtered table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_a, id_b -- the two supervoxel IDs\n",
    "# xa, ya, za -- point from which segmentation of 'a' was started, 8 nm coordinates\n",
    "# xb, yb, zb -- point from which segmentation of 'b' was started, 8 nm coordinates\n",
    "# caa, cab, cba, cbb -- cXY means: fraction of voxels from the original segment Y recovered when seeding from X\n",
    "# iou -- Jaccard index of the two local segmentations\n",
    "# da, db -- dX means: fraction of voxels that changed value from >0.8 to <0.5 when segmenting & seeding from X;\n",
    "#                     the higher this value is, the more \"internally inconsistent\" the segmentation resolution\n",
    "#                     potentially is; higher thresholds for iou, cXY might be warranted\n",
    "\n",
    "%time df = pd.DataFrame(np.load('combined-filtered-table.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.load('32nm/data-00000-of-00100.npy'))\n",
    "# print(\"Selecting retired supervoxels\")\n",
    "# rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "# print(f\"Found {rows_to_fix.sum()} rows\")\n",
    "# df_to_fix = df[rows_to_fix]\n",
    "# df_to_fix.iloc[37:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repair_coords_on_splits('32nm/data-00000-of-00100.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sc.parallelize(orig_npy_paths).foreach(repair_coords_on_splits)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_npy_paths = (  sorted(glob.glob('split-coords-fixed/32nm/*.npy'))\n",
    "                   + sorted(glob.glob('split-coords-fixed/16nm/*.npy'))\n",
    "                   + sorted(glob.glob('split-coords-fixed/8nm/*.npy')))\n",
    "fixed_npy_paths = list(map(os.path.abspath, fixed_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 10.6 ms, total: 36.7 ms\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def count_unfixable(npy_path):\n",
    "    return (np.load(npy_path)['za'] < 0).sum()\n",
    "unfixable_count = sc.parallelize(fixed_npy_paths).map(count_unfixable).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel table SVs from init agglo to current master\n",
    "(and drop bad edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('updated-tables/32nm', exist_ok=True)\n",
    "os.makedirs('updated-tables/16nm', exist_ok=True)\n",
    "os.makedirs('updated-tables/8nm', exist_ok=True)\n",
    "\n",
    "# Replace old SV ids with updated IDs by sampling from those coordinates.\n",
    "def remap_split_svs(npy_path):\n",
    "    df = pd.DataFrame(np.load(npy_path))\n",
    "    assert df['id_a'].dtype == np.uint64\n",
    "    assert df['id_b'].dtype == np.uint64\n",
    "\n",
    "    retired_svs # Reference this variable to ensure that it gets captured when pickling this function.\n",
    "    rows_to_fix = df.eval('(id_a in @retired_svs) or (id_b in @retired_svs)')\n",
    "\n",
    "    fixed_ids = []\n",
    "    df_to_fix = df[rows_to_fix]\n",
    "    for row in tqdm(df_to_fix.itertuples(), total=len(df_to_fix)):\n",
    "        id_a, id_b = row.id_a, row.id_b\n",
    "        if id_a in retired_svs:\n",
    "            id_a = fetch_label_for_coordinate(analysis_node, (row.za, row.ya, row.xa), supervoxels=True)\n",
    "        if id_b in retired_svs:\n",
    "            id_b = fetch_label_for_coordinate(analysis_node, (row.zb, row.yb, row.xb), supervoxels=True)\n",
    "        fixed_ids.append( (id_a, id_b) )\n",
    "\n",
    "    df.loc[rows_to_fix, ['id_a', 'id_b']] = np.array(fixed_ids, np.uint64)\n",
    "    assert df['id_a'].dtype == np.uint64\n",
    "    assert df['id_b'].dtype == np.uint64\n",
    "\n",
    "    parts = npy_path.split('/')\n",
    "    assert parts[-3] == 'split-coords-fixed'\n",
    "    parts[-3] = 'updated-tables'\n",
    "    new_npy_path = '/'.join(parts)\n",
    "    np.save(new_npy_path, df.to_records(index=False))\n",
    "\n",
    "    return rows_to_fix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap_split_svs(fixed_npy_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 204 ms, sys: 46.3 ms, total: 250 ms\n",
      "Wall time: 13min 29s\n"
     ]
    }
   ],
   "source": [
    "%time updated_row_count = sc.parallelize(fixed_npy_paths).map(remap_split_svs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081409"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching http://emdata3:8900/api/node/662e/segmentation/mappings...\n",
      "Fetching http://emdata3:8900/api/node/662e/segmentation/mappings took 0:00:31.333353\n",
      "Parsing mapping...\n",
      "Parsing mapping took 0:00:07.734782\n"
     ]
    }
   ],
   "source": [
    "mapping = fetch_mappings(analysis_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 471 ms, total: 24.4 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%time mapper = LabelMapper(mapping.index.values, mapping.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('filtered-tables/32nm', exist_ok=True)\n",
    "os.makedirs('filtered-tables/16nm', exist_ok=True)\n",
    "os.makedirs('filtered-tables/8nm', exist_ok=True)\n",
    "\n",
    "# Replace old SV ids with updated IDs by sampling from those coordinates.\n",
    "def apply_mapping_and_filter_to_partition(paths):\n",
    "    # Must create mapper here since it cannot be pickled.\n",
    "    mapper = LabelMapper(mapping.index.values, mapping.values)\n",
    "\n",
    "    def apply_mapping_and_filter(npy_path):\n",
    "        df = pd.DataFrame(np.load(npy_path))\n",
    "\n",
    "        # A bug above caused the type to be int64. Fix that now.\n",
    "        df['id_a'] = df['id_a'].astype(np.uint64)\n",
    "        df['id_b'] = df['id_b'].astype(np.uint64)\n",
    "        \n",
    "        df['body_a'] = mapper.apply(df['id_a'].values, allow_unmapped=True)\n",
    "        df['body_b'] = mapper.apply(df['id_b'].values, allow_unmapped=True)\n",
    "\n",
    "        important_bodies # Referenced to ensure capture in this closure\n",
    "\n",
    "        # Drop internal edges,\n",
    "        # Filter for important bodies (on at least one end -- capture 1-hop and 2-hop)\n",
    "        q = '(body_a != body_b) and ((body_a in @important_bodies) or (body_b in @important_bodies))'\n",
    "        df.query(q, inplace=True)\n",
    "\n",
    "        parts = npy_path.split('/')\n",
    "        assert parts[-3] == 'updated-tables'\n",
    "        parts[-3] = 'filtered-tables'\n",
    "        new_npy_path = '/'.join(parts)\n",
    "        np.save(new_npy_path, df.to_records(index=False))\n",
    "\n",
    "        return len(df)\n",
    "    \n",
    "    return list(map(apply_mapping_and_filter, paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_npy_paths = (  sorted(glob.glob('updated-tables/32nm/*.npy'))\n",
    "                     + sorted(glob.glob('updated-tables/16nm/*.npy'))\n",
    "                     + sorted(glob.glob('updated-tables/8nm/*.npy')))\n",
    "updated_npy_paths = list(map(os.path.abspath, updated_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 4.18 s, total: 27.7 s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "filtered_row_count = (sc.parallelize(updated_npy_paths)\n",
    "                        .mapPartitions(apply_mapping_and_filter_to_partition)\n",
    "                        .sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755536110\n"
     ]
    }
   ],
   "source": [
    "print(filtered_row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_npy_paths = (  sorted(glob.glob('filtered-tables/32nm/*.npy'))\n",
    "                      + sorted(glob.glob('filtered-tables/16nm/*.npy'))\n",
    "                      + sorted(glob.glob('filtered-tables/8nm/*.npy')))\n",
    "filtered_npy_paths = list(map(os.path.abspath, filtered_npy_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:14<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_table = np.concatenate(list(map(np.load, tqdm(filtered_npy_paths))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755.53611 M\n",
      "64.22056935 GB\n"
     ]
    }
   ],
   "source": [
    "print(combined_table.shape[0] / 1e6, \"M\")\n",
    "print(combined_table.nbytes / 1e9, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame(combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 s, sys: 1min 7s, total: 1min 10s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%time np.save('combined-filtered-table.npy', combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;27m16nm\u001b[0m/                               \u001b[38;5;27mnotebook-cluster--20180723.094528\u001b[0m/\r\n",
      "\u001b[38;5;27m32nm\u001b[0m/                               \u001b[38;5;27mnotebook-cluster--20180723.094651\u001b[0m/\r\n",
      "\u001b[38;5;27m8nm\u001b[0m/                                \u001b[38;5;27mnotebook-cluster--20180723.180735\u001b[0m/\r\n",
      "bodies-0.5-from-neuprint-52f9.csv   spark-focused.ipynb\r\n",
      "\u001b[38;5;27mfiltered-tables\u001b[0m/                    \u001b[38;5;27msplit-coords-fixed\u001b[0m/\r\n",
      "\u001b[38;5;27mnotebook-cluster--20180722.201030\u001b[0m/  \u001b[38;5;27mupdated-tables\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
